{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "import requests\n",
    "from bs4 import Tag, NavigableString, BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store all players' links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_players_urls(main_url, competitions_url, wanted_seasons):\n",
    "    \"\"\"\n",
    "    Extracts all big 5 European player urls from the main competition \n",
    "    url if they have played in the wanted seasons.\n",
    "    \n",
    "    Arguments:\n",
    "        main_url: string to main fbreb url\n",
    "        competitions_url: string to competitions main webpage\n",
    "        wanted_seasons: list with strings of wanted seasons in format \n",
    "                        '2020-2021'\n",
    "                        \n",
    "    Returns:\n",
    "        all_players_urls: list with player urls.\n",
    "    \"\"\"\n",
    "    html = urlopen(competitions_url)\n",
    "    bs_competitions = BeautifulSoup(html)\n",
    "\n",
    "    # Find big 5 leagues urls\n",
    "    big5_urls = []\n",
    "    big5_table = bs_competitions.find_all('table', {'class':'sortable'})[2]\n",
    "    big5_rows = big5_table.tbody\n",
    "\n",
    "    for row in big5_rows:\n",
    "        if not isinstance(row, NavigableString):\n",
    "            big5_urls.append(main_url + row.a['href'])\n",
    "\n",
    "    # Find the wanted seasons' urls of the big5\n",
    "    league_seasons_urls = []\n",
    "    for league_url in big5_urls[:5]: # avoid big 5 european leagues combined\n",
    "        # Open html\n",
    "        html = urlopen(league_url)\n",
    "        bs_league = BeautifulSoup(html)\n",
    "        # Extract seasons' urls\n",
    "        seasons_table = bs_league.find('tbody').find_all('th')\n",
    "        for season in seasons_table:\n",
    "            if season.text in wanted_seasons:\n",
    "                league_seasons_urls.append(main_url + season.a['href'])\n",
    "\n",
    "    # Find all the teams per season\n",
    "    season_team_urls = []\n",
    "    for season_url in league_seasons_urls:\n",
    "        # Open html\n",
    "        html = urlopen(season_url)\n",
    "        bs_season = BeautifulSoup(html)\n",
    "        # Extract teams' urls\n",
    "        teams_table = bs_season.find('table', {'class', 'stats_table'}).find_all('tr')\n",
    "        for team in teams_table:\n",
    "            team_cell = team.find('td')\n",
    "            if team_cell != None:\n",
    "                season_team_urls.append(main_url + team_cell.a['href'])\n",
    "\n",
    "    # Find all players' urls\n",
    "    all_players_urls = []\n",
    "    for season_team_url in season_team_urls:\n",
    "        # Open html\n",
    "        html = urlopen(season_team_url)\n",
    "        bs_team_season = BeautifulSoup(html)\n",
    "        # Extract players' urls\n",
    "        players_table = bs_team_season.find('table', {'class', 'stats_table'}).tbody.find_all('tr')\n",
    "        for player_row in players_table:\n",
    "            player_url = main_url + player_row.th.a['href']\n",
    "            if player_url not in all_players_urls:\n",
    "                all_players_urls.append(player_url)\n",
    "    \n",
    "    return all_players_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Players' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_players_brief(bs_player):\n",
    "    \"\"\"\n",
    "    Extracts biographical and miscellaneous information of the players.\n",
    "    \n",
    "    Argument: \n",
    "        bs_player: BeautifoulSoup object of the player's html.\n",
    "    \n",
    "    Returns: tuple with the extracted player information\n",
    "    \"\"\"\n",
    "    # Player's brief\n",
    "    player_brief = bs_player.find('div', {'id': 'info'}).div\n",
    "\n",
    "    # Extract image from brief\n",
    "    image = None\n",
    "    ############################################################################\n",
    "    #image_line = player_brief.find('div', {'class': 'media-item'})\n",
    "    #if image_line != None:\n",
    "    #    image = image_line.img['src']\n",
    "    ################################################################################\n",
    "\n",
    "    # Extract name from brief\n",
    "    name = player_brief.span.string\n",
    "    first_name = name.split(\" \")[0]\n",
    "\n",
    "    # Extract all brief info below the name\n",
    "    all_lines_brief = player_brief.find_all('p')\n",
    "\n",
    "    complete_name = name\n",
    "    position, footed = None, None\n",
    "    height, weight = None, None\n",
    "    birth_year = None\n",
    "    born_country, citizenship = None, None\n",
    "    national, youth_national = None, None\n",
    "    \n",
    "    # complete name\n",
    "    c_name_line = player_brief.find('h1', {'itemprop':'name'}).next_sibling.next_sibling\n",
    "    if not \"Position:\" in c_name_line.text:\n",
    "        complete_name = c_name_line.text\n",
    "    else:\n",
    "        complete_name = name\n",
    "    \n",
    "    # height\n",
    "    height = player_brief.find('span', {'itemprop':'height'})\n",
    "    if height != None:\n",
    "        height = height.text.strip()\n",
    "\n",
    "    # weight\n",
    "    weight = player_brief.find('span', {'itemprop':'weight'})\n",
    "    if weight != None:\n",
    "        weight = weight.text.strip()\n",
    "\n",
    "    # birthday\n",
    "    birthday = player_brief.find('span', {'itemprop':'birthDate'})\n",
    "    if birthday != None:\n",
    "        birthday= birthday.text.strip().split(\" \")[-1]  \n",
    "\n",
    "    # Other fields\n",
    "    for line in all_lines_brief:\n",
    "            \n",
    "        # field position and footed\n",
    "        if \"Position\" in line.text:\n",
    "            position_fooded = line.text\n",
    "\n",
    "            if \"Footed\" in position_fooded:\n",
    "                position = re.search('[A-Z]+(-[A-Z]*)*', position_fooded.split(\"Position: \")\n",
    "                                     [1]).group(0)\n",
    "                footed = re.search('[A-Za-z]+', position_fooded.split(\"Footed: \")\n",
    "                                   [1]).group(0)\n",
    "            else:\n",
    "                position = position_fooded.split(\"Position: \")[1].strip()\n",
    "                footed = None\n",
    "        \n",
    "        # Country data comes from different origins because depending on the player\n",
    "        # there will be or not info in the birthplace, citizenship and national team fields\n",
    "        # Birthday and country\n",
    "        if \"Born:\" in line.text:\n",
    "            birth_year = re.search('[0-9]+\\n', line.text)\n",
    "            if birth_year != None:\n",
    "                birth_year = birth_year.group(0).strip()\n",
    "                if \"in\" in line.text:\n",
    "                    born_country = re.search(', [A-Za-z]+\\n', line.text)\n",
    "                    if isinstance(born_country, re.Match):\n",
    "                        born_country = born_country.group(0).split(\" \")[1].strip()\n",
    "\n",
    "        # Citizenship\n",
    "        if \"Citizenship\" in line.text:\n",
    "            citizenship = line.a\n",
    "            if citizenship != None:\n",
    "                 citizenship = citizenship.text\n",
    "        \n",
    "        # Youth national team\n",
    "        if \"Youth National Team\" in line.text:\n",
    "            youth_national = line.a\n",
    "            if youth_national != None:\n",
    "                 youth_national = youth_national.text\n",
    "                    \n",
    "        # National team\n",
    "        if \"National Team\" in line.text:\n",
    "            national = line.a\n",
    "            if national != None:\n",
    "                 national = national.text\n",
    "    \n",
    "    # Final country field value\n",
    "    country = born_country\n",
    "    if country is None:\n",
    "        country = citizenship\n",
    "        if country is None:\n",
    "            country = youth_national\n",
    "            if country is None:\n",
    "                country = national\n",
    "                \n",
    "    ##################################################################################\n",
    "    # Extract trophies\n",
    "    # player_trophies = bs_player.find('div', {'id': 'info'}).ul\n",
    "    # player_trophies.find_all('li', {'class': 'important poptip'})\n",
    "    # player_trophies.find_all('li', {'class': 'important all_star poptip'})\n",
    "    ##################################################################################\n",
    "\n",
    "    return (image, name, complete_name, position, footed, height, weight, birthday, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " 'Coke',\n",
       " 'Jorge And√∫jar Moreno',\n",
       " 'DF',\n",
       " 'Right',\n",
       " '182cm',\n",
       " '78kg',\n",
       " '1987',\n",
       " 'Spain')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing multiple players\n",
    "url =\"https://fbref.com/en/players/31c69ef1/Ruben-Dias\"\n",
    "url2  =\"https://fbref.com/en/players/6d8f8441/scout/365_euro/Coke-Scouting-Report\"\n",
    "\n",
    "html = urlopen(url2)\n",
    "bs_player = BeautifulSoup(html)\n",
    "\n",
    "extract_players_brief(bs_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_standard_stats_table(bs_player, wanted_seasons):\n",
    "    \"\"\"\n",
    "    Extracts the league stats of the players. Only stats in wanted seasons\n",
    "    \n",
    "    Argument: \n",
    "        bs_player: BeautifoulSoup object of the player's html.\n",
    "        wanted_seasons: list with strings of wanted seasons in format \n",
    "                        '2020-2021'\n",
    "    \n",
    "    Returns: dataframe with the extracted player information\n",
    "    \"\"\"\n",
    "    # Filter standard stats for all competitions\n",
    "    standard_stats = bs_player.find(id = 'div_stats_standard_dom_lg')\n",
    "\n",
    "    if standard_stats != None:\n",
    "\n",
    "        # Extract standard_stats column names\n",
    "        head_table = standard_stats.thead.find_all('tr')\n",
    "        column_names = head_table[1].text.strip().split(\" \")[:17]\n",
    "\n",
    "        # Extract standard_stats data (only playing time and performance metrics)\n",
    "        all_data = []\n",
    "        rows_table = standard_stats.tbody.find_all('tr') \n",
    "        for data in rows_table:\n",
    "            season = data.th.text\n",
    "            if season in wanted_seasons:\n",
    "                age = data.find('td', {'data-stat':'age'}).text\n",
    "                squad = data.find('td', {'data-stat':'squad'}).text\n",
    "                country = data.find('td', {'data-stat':'country'}).text.split(\" \")[1]\n",
    "                comp = data.find('td', {'data-stat':'comp_level'}).a.text\n",
    "                LgRank = data.find('td', {'data-stat':'lg_finish'}).text\n",
    "                # Playing time\n",
    "                MP = data.find('td', {'data-stat':'games'}).text\n",
    "                Starts = data.find('td', {'data-stat':'games_starts'}).text\n",
    "                Min = data.find('td', {'data-stat':'minutes'}).text\n",
    "                var_90s = data.find('td', {'data-stat':'minutes_90s'}).text\n",
    "                # Performance\n",
    "                Gls = data.find('td', {'data-stat':'goals'}).text\n",
    "                Ast = data.find('td', {'data-stat':'assists'}).text\n",
    "                G_PK = data.find('td', {'data-stat':'goals_pens'}).text\n",
    "                PK = data.find('td', {'data-stat':'pens_made'}).text\n",
    "                PKatt = data.find('td', {'data-stat':'pens_att'}).text\n",
    "                CrdY = data.find('td', {'data-stat':'cards_yellow'}).text\n",
    "                CrdR = data.find('td', {'data-stat':'cards_red'}).text\n",
    "\n",
    "                all_data.append([season, age, squad, country, comp, LgRank, MP, Starts, Min, \n",
    "                  var_90s, Gls, Ast, G_PK, PK, PKatt, CrdY, CrdR])\n",
    "\n",
    "        df = pd.DataFrame(all_data, columns=column_names)\n",
    "        return df\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if player is in wanted seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to extract brief and stats data if players have participated in the wanted seasons leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_player_if_in_seasons(bs_player, wanted_seasons):\n",
    "    \"\"\"\n",
    "    Checks if a player played in league seasons present in the list\n",
    "    wanted_seasons.\n",
    "    \n",
    "    Arguments:\n",
    "        bs_player: BeautifoulSoup object of the player's html.\n",
    "        wanted_seasons: list with strings of wanted seasons in format \n",
    "                        '2020-2021'    \n",
    "    \n",
    "    Returns:\n",
    "        Boolean: true if player played the league in the wanted seasons.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Filter standard stats for all competitions\n",
    "    standard_stats = bs_player.find(id = 'div_stats_standard_dom_lg')\n",
    "    \n",
    "    if standard_stats != None:\n",
    "        rows_table = standard_stats.tbody.find_all('tr')\n",
    "        for data in rows_table:\n",
    "            season = data.th.text\n",
    "            if season in wanted_seasons:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_data(wanted_seasons, player_urls_file):\n",
    "    \"\"\"\n",
    "    Create final dataframe with all players league stats for the wanted\n",
    "    seasons. \n",
    "    \n",
    "    Arguments:\n",
    "        wanted_seasons: list with strings of wanted seasons in format \n",
    "                        '2020-2021'    \n",
    "        players_urls_file: file with all players urls.\n",
    "    \n",
    "    Returns:\n",
    "        df: dataframe with all players' information\n",
    "        down_urls: list of player urls already downloaded\n",
    "        not_wanted_urls: list of player urls that are not in wanted \n",
    "                        seasons leagues\n",
    "    \"\"\"\n",
    "    \n",
    "    df = None\n",
    "    \n",
    "    # Get players urls\n",
    "    players_urls = []\n",
    "    with open(player_urls_file, 'r') as file:\n",
    "        for row in file:\n",
    "            players_urls.append(row.strip())\n",
    "    \n",
    "     \n",
    "    num_urls = len(players_urls) # For printing\n",
    "    down_urls = [] # Player urls already downloaded\n",
    "    not_wanted_urls = [] # Player urls that are not in wanted seasons\n",
    "    \n",
    "    # Extract data from players\n",
    "    for num, url in enumerate(players_urls):\n",
    "        \n",
    "        # Print scraping state\n",
    "        print(\"\\rPlayer number {}/{}.\".format(num, num_urls), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # html to beautifulsoup\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "            bs_player = BeautifulSoup(html)\n",
    "            down_urls.append(url)\n",
    "        except HTTPError as e:\n",
    "            print(e)\n",
    "            return df, down_urls, not_wanted_urls\n",
    "        except URLError as e:\n",
    "            print(e)\n",
    "            return df, down_urls, not_wanted_urls\n",
    "        except:\n",
    "            return df, down_urls, not_wanted_urls\n",
    "            \n",
    "\n",
    "        # check if player is in wanted league seasons\n",
    "        is_wanted = check_player_if_in_seasons(bs_player, wanted_seasons)\n",
    "\n",
    "        if is_wanted:\n",
    "\n",
    "            # brief_data\n",
    "            brief_data = extract_players_brief(bs_player)    \n",
    "            image, name, complete_name, position = brief_data[0:4]\n",
    "            footed, height, weight, birth_year, country = brief_data[4:]\n",
    "\n",
    "            # standard stats\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                df = extract_standard_stats_table(bs_player, wanted_seasons)\n",
    "                #df.insert(17, 'image', image)\n",
    "                df.insert(0, 'player_country', country)\n",
    "                df.insert(0, 'birth_year', birth_year)\n",
    "                df.insert(0, 'weight', weight)\n",
    "                df.insert(0, 'height', height)\n",
    "                df.insert(0, 'footed', footed)\n",
    "                df.insert(0, 'position', position)\n",
    "                df.insert(0, 'c_name', complete_name)\n",
    "                df.insert(0, 'name', name)\n",
    "\n",
    "\n",
    "            else:\n",
    "                df2 = extract_standard_stats_table(bs_player, wanted_seasons)\n",
    "                #df2.insert(17, 'image', image)\n",
    "                df2.insert(0, 'player_country', country)\n",
    "                df2.insert(0, 'birth_year', birth_year)\n",
    "                df2.insert(0, 'weight', weight)\n",
    "                df2.insert(0, 'height', height)\n",
    "                df2.insert(0, 'footed', footed)\n",
    "                df2.insert(0, 'position', position)\n",
    "                df2.insert(0, 'c_name', complete_name)\n",
    "                df2.insert(0, 'name', name)\n",
    "                df = df.append(df2)\n",
    "        else:\n",
    "            not_wanted_urls.append(url)\n",
    "            \n",
    "        if num % 100 == 0 and num != 0:\n",
    "        \n",
    "            df.to_csv('players_stats.csv')\n",
    "        \n",
    "    return df, down_urls, not_wanted_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://fbref.com\"\n",
    "competitions_url = \"https://fbref.com/en/comps/\"\n",
    "\n",
    "wanted_seasons = ['2010-2011', '2011-2012', '2012-2013', '2013-2014', '2014-2015', \n",
    "                  '2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', \n",
    "                  '2020-2021']\n",
    "\n",
    "# Extract all players' links\n",
    "player_urls_file = 'links_all_players.txt'\n",
    "\n",
    "# Get players urls\n",
    "all_players_urls = []\n",
    "\n",
    "with open(player_urls_file, 'r') as file:\n",
    "    for row in file:\n",
    "        all_players_urls.append(row.strip())\n",
    "        \n",
    "        \n",
    "if len(all_players_urls) == 0:\n",
    "    # Extract and store players urls on a file\n",
    "    all_players_urls = extract_players_urls(main_url, competitions_url, wanted_seasons)\n",
    "\n",
    "    with open(player_urls_file, 'w') as file:\n",
    "        for player_link in all_players_urls:\n",
    "            player_link += \"\\n\"\n",
    "            file.write(player_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player number 3288/3289."
     ]
    }
   ],
   "source": [
    "df, down_urls, not_wanted_urls = extract_all_data(wanted_seasons, player_urls_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
